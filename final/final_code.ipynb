{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DubdTLh8UbFJ"
   },
   "source": [
    "# AML Final -- Fake News Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import scipy\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7rdytnVUbFN"
   },
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train (2).csv', delimiter=';')\n",
    "train = train[['title', 'text', 'label']]\n",
    "test = pd.read_csv('test (1).csv', delimiter=';')\n",
    "test = test[['title', 'text', 'label']]\n",
    "val = pd.read_csv('evaluation.csv', delimiter=';')\n",
    "val = val[['title', 'text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "train['text'] = train['text'].str.lower()\n",
    "test['text'] = test['text'].str.lower()\n",
    "val['text'] = val['text'].str.lower()\n",
    "\n",
    "# drop na\n",
    "train = train.dropna(subset=['text'])\n",
    "test = test.dropna(subset=['text'])\n",
    "val = val.dropna(subset=['text'])\n",
    "\n",
    "# lemmatize\n",
    "wnl = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([wnl.lemmatize(word) for word in text.split()])\n",
    "\n",
    "train['text'] = train['text'].apply(lemmatize_text)\n",
    "test['text'] = test['text'].apply(lemmatize_text)\n",
    "val['text'] = val['text'].apply(lemmatize_text)\n",
    "\n",
    "# remove punctuation\n",
    "train['text'] = train['text'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n",
    "test['text'] = test['text'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n",
    "val['text'] = val['text'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x: contractions.fix(x))\n",
    "test['text'] = test['text'].apply(lambda x: contractions.fix(x))\n",
    "val['text'] = val['text'].apply(lambda x: contractions.fix(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \"\"\"\n",
    "    From assignment template code\n",
    "    \"\"\"\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', str(text), flags=re.MULTILINE)\n",
    "    texter = re.sub(r\"<br />\", \" \", text)\n",
    "    texter = re.sub(r\"&quot;\", \"\\\"\",texter)\n",
    "    texter = re.sub('&#39;', \"\\\"\", texter)\n",
    "    texter = re.sub('\\n', \" \", texter)\n",
    "    texter = re.sub(' u ',\" you \", texter)\n",
    "    texter = re.sub('`',\"\", texter)\n",
    "    texter = re.sub(' +', ' ', texter)\n",
    "    texter = re.sub(r\"(!)\\1+\", r\"!\", texter)\n",
    "    texter = re.sub(r\"(\\?)\\1+\", r\"?\", texter)\n",
    "    texter = re.sub('&amp;', 'and', texter)\n",
    "    texter = re.sub('\\r', ' ',texter)\n",
    "    #added substitutions\n",
    "\n",
    "    #***********added substitutions***********\n",
    "    # remove all the special characters\n",
    "    texter = re.sub(r'\\W', ' ', texter)\n",
    "    # remove all single characters\n",
    "    texter = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', texter)\n",
    "    # Remove single characters from the start\n",
    "    texter = re.sub(r'\\^[a-zA-Z]\\s+', ' ', texter)\n",
    "    # Remove numbers\n",
    "    texter = re.sub(r'\\d+', ' ', texter)\n",
    "    # Converting to Lowercase\n",
    "    texter = texter.lower()\n",
    "    # Remove punctuation\n",
    "    texter = re.sub(r'[^\\w\\s]', ' ', texter)\n",
    "    # Remove parentheses\n",
    "    texter = re.sub(r'\\([^)]*\\)', ' ', texter)\n",
    "    # Remove single quotes\n",
    "    texter = re.sub(r'\\'', ' ', texter)\n",
    "    # Substituting multiple spaces with single space\n",
    "    texter = re.sub(r'\\s+', ' ', texter, flags=re.I)\n",
    "\n",
    "    clean = re.compile('<.*?>')\n",
    "    texter = texter.encode('ascii', 'ignore').decode('ascii')\n",
    "    texter = re.sub(clean, '', texter)\n",
    "    if texter == \"\":\n",
    "        texter = \"\"\n",
    "    return texter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(clean)\n",
    "test['text'] = test['text'].apply(clean)\n",
    "val['text'] = val['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ramallah west bank reuters palestinian switche...\n",
       "1        beijing reuters us presidentelect donald trump...\n",
       "2        while the controversy over trump personal tax ...\n",
       "3        beijing reuters trip to beijing last week by z...\n",
       "4        there ha never been more uncourageous person i...\n",
       "                               ...                        \n",
       "24348    mexico city reuters key committee in mexico se...\n",
       "24349    if she not toast now then we re in bigger trou...\n",
       "24350    kremlin nato wa created for agression russia t...\n",
       "24351    dallas cowboy star wide receiver dez bryant to...\n",
       "24352    update nordstrom stock closed up slightly toda...\n",
       "Name: text, Length: 24353, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrBHC3acoAdY"
   },
   "source": [
    "# Logistic Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      3753\n",
      "           1       0.98      0.97      0.97      4364\n",
      "\n",
      "    accuracy                           0.97      8117\n",
      "   macro avg       0.97      0.97      0.97      8117\n",
      "weighted avg       0.97      0.97      0.97      8117\n",
      "\n",
      "\n",
      "Accuracy Score: 0.9727731920660343\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3647  106]\n",
      " [ 115 4249]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# vectorize\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # tfidf\n",
    "x_train = tfidf_vectorizer.fit_transform(train['text'])  # fit/transform on training data\n",
    "x_test = tfidf_vectorizer.transform(test['text'])  # only transform test data\n",
    "\n",
    "# train logreg\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "y_train = train['label']\n",
    "log_reg.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_test = test['label']\n",
    "y_pred = log_reg.predict(x_test)\n",
    "\n",
    "# eval\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpRIwyxLUbFP"
   },
   "source": [
    "# Embedding / Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb0a0d769844746a916995b23cfc421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503b0b803aae4fd29fa12cce83547324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938961b025e945abb14120b79b97b695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d344d4da44141828c601104a1b8c391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1ec134c2b147758b877f19ce36cbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "# load distilbert\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text_list, tokenizer, max_length=128):\n",
    "    \"\"\"Tokenizes and pads text data.\"\"\"\n",
    "    return tokenizer(\n",
    "        text_list,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize all datasets\n",
    "train_tokenized = tokenize_text(train['text'].tolist(), tokenizer)\n",
    "val_tokenized = tokenize_text(val['text'].tolist(), tokenizer)\n",
    "test_tokenized = tokenize_text(test['text'].tolist(), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfCQAEbuUbFR"
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class DistilBertForFakeNewsClassification(nn.Module):\n",
    "    def __init__(self, bert_model, num_labels):\n",
    "        super(DistilBertForFakeNewsClassification, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(768, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.fc(cls_embedding)\n",
    "        return logits\n",
    "\n",
    "# init model\n",
    "bert_classifier = DistilBertForFakeNewsClassification(distilbert_model, num_labels=1).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvU1Pj6cUbFS"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# optimzier/loss\n",
    "optimizer = AdamW(bert_classifier.parameters(), lr=2e-5)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0d/vbzvfj397ts7gxw_gwbc669m0000gn/T/ipykernel_8360/3912257642.py:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1729646995093/work/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  val_predictions = (torch.tensor(val_logits_list).squeeze() > 0).float().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Training Loss: 0.0652 | Validation Accuracy: 0.9866\n",
      "Epoch 2/5 | Training Loss: 0.0304 | Validation Accuracy: 0.9868\n",
      "Epoch 3/5 | Training Loss: 0.0145 | Validation Accuracy: 0.9873\n",
      "Epoch 4/5 | Training Loss: 0.0086 | Validation Accuracy: 0.9892\n",
      "Epoch 5/5 | Training Loss: 0.0053 | Validation Accuracy: 0.9887\n"
     ]
    }
   ],
   "source": [
    "# preprocess labels before  loop\n",
    "train_labels_tensor = torch.tensor(train['label'].values).unsqueeze(1).float().to(device)\n",
    "val_labels_tensor = torch.tensor(val['label'].values).unsqueeze(1).float().to(device)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    bert_classifier.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(0, len(train_tokenized['input_ids']), batch_size):\n",
    "        batch_input_ids = train_tokenized['input_ids'][i:i+batch_size].to(device)\n",
    "        batch_attention_mask = train_tokenized['attention_mask'][i:i+batch_size].to(device)\n",
    "        batch_labels = train_labels_tensor[i:i+batch_size]  # Use preprocessed labels\n",
    "\n",
    "        logits = bert_classifier(batch_input_ids, batch_attention_mask)\n",
    "        loss = loss_fn(logits, batch_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / (len(train_tokenized['input_ids']) / batch_size)\n",
    "\n",
    "    # validation\n",
    "    bert_classifier.eval()\n",
    "    val_logits_list = []\n",
    "    val_labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(val_tokenized['input_ids']), batch_size):\n",
    "            val_input_ids = val_tokenized['input_ids'][i:i+batch_size].to(device)\n",
    "            val_attention_mask = val_tokenized['attention_mask'][i:i+batch_size].to(device)\n",
    "            val_labels = val_labels_tensor[i:i+batch_size]  #  preprocessed labels\n",
    "\n",
    "            logits = bert_classifier(val_input_ids, val_attention_mask)\n",
    "            val_logits_list.extend(logits.cpu().numpy())\n",
    "            val_labels_list.extend(val_labels.cpu().numpy())  # to numpy for evaluation\n",
    "\n",
    "    val_predictions = (torch.tensor(val_logits_list).squeeze() > 0).float().numpy()\n",
    "    val_accuracy = accuracy_score(val_labels_list, val_predictions)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Training Loss: {avg_train_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9884193667611186\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3716   37]\n",
      " [  57 4307]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Fake News       0.98      0.99      0.99      3753\n",
      "   Real News       0.99      0.99      0.99      4364\n",
      "\n",
      "    accuracy                           0.99      8117\n",
      "   macro avg       0.99      0.99      0.99      8117\n",
      "weighted avg       0.99      0.99      0.99      8117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "bert_classifier.eval()\n",
    "test_logits_list = []\n",
    "test_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_tokenized['input_ids']), batch_size):\n",
    "        test_input_ids = test_tokenized['input_ids'][i:i+batch_size].to(device)\n",
    "        test_attention_mask = test_tokenized['attention_mask'][i:i+batch_size].to(device)\n",
    "        test_labels = test['label'][i:i+batch_size].values\n",
    "\n",
    "        logits = bert_classifier(test_input_ids, test_attention_mask)\n",
    "        test_logits_list.extend(logits.cpu().numpy())\n",
    "        test_labels_list.extend(test_labels)\n",
    "\n",
    "# logits to predictions\n",
    "test_predictions = (torch.tensor(test_logits_list).squeeze() > 0).float().numpy()\n",
    "\n",
    "# test accuracy\n",
    "test_accuracy = accuracy_score(test_labels_list, test_predictions)\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels_list, test_predictions)\n",
    "\n",
    "# classification report\n",
    "clf_report = classification_report(test_labels_list, test_predictions, target_names=['Fake News', 'Real News'])\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(clf_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIrK9doUAr_1"
   },
   "source": [
    "# Combine Train + Val and Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine train and val set for final model\n",
    "final_train = pd.concat([train, val], ignore_index=True)\n",
    "\n",
    "final_tokenized = tokenize_text(final_train['text'].tolist(), tokenizer)\n",
    "final_labels_tensor = torch.tensor(final_train['label'].values).unsqueeze(1).float().to(device)\n",
    "\n",
    "# re init model\n",
    "bert_classifier = DistilBertForFakeNewsClassification(distilbert_model, num_labels=1).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Training Loss: 0.0169\n",
      "Epoch 2/5 | Training Loss: 0.0078\n",
      "Epoch 3/5 | Training Loss: 0.0048\n",
      "Epoch 4/5 | Training Loss: 0.0029\n",
      "Epoch 5/5 | Training Loss: 0.0030\n"
     ]
    }
   ],
   "source": [
    "final_labels_tensor = torch.tensor(final_train['label'].values).unsqueeze(1).float().to(device)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    bert_classifier.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(0, len(final_tokenized['input_ids']), batch_size):\n",
    "        batch_input_ids = final_tokenized['input_ids'][i:i+batch_size].to(device)\n",
    "        batch_attention_mask = final_tokenized['attention_mask'][i:i+batch_size].to(device)\n",
    "        batch_labels = final_labels_tensor[i:i+batch_size]  #  combined labels\n",
    "\n",
    "        logits = bert_classifier(batch_input_ids, batch_attention_mask)\n",
    "        loss = loss_fn(logits, batch_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / (len(final_tokenized['input_ids']) / batch_size)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Training Loss: {avg_train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O28CLg0yUbFT"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.9864\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3682   71]\n",
      " [  39 4325]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Fake News       0.99      0.98      0.99      3753\n",
      "   Real News       0.98      0.99      0.99      4364\n",
      "\n",
      "    accuracy                           0.99      8117\n",
      "   macro avg       0.99      0.99      0.99      8117\n",
      "weighted avg       0.99      0.99      0.99      8117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# eval on test set\n",
    "bert_classifier.eval()\n",
    "test_logits_list = []\n",
    "test_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_tokenized['input_ids']), batch_size):\n",
    "        test_input_ids = test_tokenized['input_ids'][i:i+batch_size].to(device)\n",
    "        test_attention_mask = test_tokenized['attention_mask'][i:i+batch_size].to(device)\n",
    "        test_labels = test['label'][i:i+batch_size].values\n",
    "\n",
    "        logits = bert_classifier(test_input_ids, test_attention_mask)\n",
    "        test_logits_list.extend(logits.cpu().numpy())\n",
    "        test_labels_list.extend(test_labels)\n",
    "\n",
    "# logits to predictions\n",
    "test_predictions = (torch.tensor(test_logits_list).squeeze() > 0).float().numpy()\n",
    "\n",
    "# test accuracy\n",
    "test_accuracy = accuracy_score(test_labels_list, test_predictions)\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels_list, test_predictions)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# classification report\n",
    "clf_report = classification_report(test_labels_list, test_predictions, target_names=['Fake News', 'Real News'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(clf_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('distilbert_fakenews_tokenizer/tokenizer_config.json',\n",
       " 'distilbert_fakenews_tokenizer/special_tokens_map.json',\n",
       " 'distilbert_fakenews_tokenizer/vocab.txt',\n",
       " 'distilbert_fakenews_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(bert_classifier.state_dict(), \"distilbert_fakenews_model.pth\")\n",
    "\n",
    "tokenizer.save_pretrained(\"distilbert_fakenews_tokenizer\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
