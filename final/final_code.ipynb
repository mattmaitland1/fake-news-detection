{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DubdTLh8UbFJ"
   },
   "source": [
    "# AML Final -- Fake News Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAH3DXmm4zR8"
   },
   "source": [
    "Note: This NB was created in Colab, so there might be issues with metadata when viewing in a jupyter NB. I am also including the \"printed\" pdf version in case the notebook can not be viewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import scipy\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7rdytnVUbFN"
   },
   "source": [
    "#Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train (2).csv', delimiter=';')\n",
    "train = train[['title', 'text', 'label']]\n",
    "test = pd.read_csv('test (1).csv', delimiter=';')\n",
    "test = test[['title', 'text', 'label']]\n",
    "val = pd.read_csv('evaluation.csv', delimiter=';')\n",
    "val = val[['title', 'text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "train['text'] = train['text'].str.lower()\n",
    "test['text'] = test['text'].str.lower()\n",
    "val['text'] = val['text'].str.lower()\n",
    "\n",
    "# drop na\n",
    "train = train.dropna(subset=['text'])\n",
    "test = test.dropna(subset=['text'])\n",
    "val = val.dropna(subset=['text'])\n",
    "\n",
    "# lemmatize\n",
    "wnl = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([wnl.lemmatize(word) for word in text.split()])\n",
    "\n",
    "train['text'] = train['text'].apply(lemmatize_text)\n",
    "test['text'] = test['text'].apply(lemmatize_text)\n",
    "val['text'] = val['text'].apply(lemmatize_text)\n",
    "\n",
    "# remove punctuation\n",
    "train['text'] = train['text'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n",
    "test['text'] = test['text'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n",
    "val['text'] = val['text'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n",
    "\n",
    "train['text'] = train['text'].apply(lambda x: contractions.fix(x))\n",
    "test['text'] = test['text'].apply(lambda x: contractions.fix(x))\n",
    "val['text'] = val['text'].apply(lambda x: contractions.fix(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \"\"\"\n",
    "    From assignment template code\n",
    "    \"\"\"\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', str(text), flags=re.MULTILINE)\n",
    "    texter = re.sub(r\"<br />\", \" \", text)\n",
    "    texter = re.sub(r\"&quot;\", \"\\\"\",texter)\n",
    "    texter = re.sub('&#39;', \"\\\"\", texter)\n",
    "    texter = re.sub('\\n', \" \", texter)\n",
    "    texter = re.sub(' u ',\" you \", texter)\n",
    "    texter = re.sub('`',\"\", texter)\n",
    "    texter = re.sub(' +', ' ', texter)\n",
    "    texter = re.sub(r\"(!)\\1+\", r\"!\", texter)\n",
    "    texter = re.sub(r\"(\\?)\\1+\", r\"?\", texter)\n",
    "    texter = re.sub('&amp;', 'and', texter)\n",
    "    texter = re.sub('\\r', ' ',texter)\n",
    "    #added substitutions\n",
    "\n",
    "    #***********added substitutions***********\n",
    "    # remove all the special characters\n",
    "    texter = re.sub(r'\\W', ' ', texter)\n",
    "    # remove all single characters\n",
    "    texter = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', texter)\n",
    "    # Remove single characters from the start\n",
    "    texter = re.sub(r'\\^[a-zA-Z]\\s+', ' ', texter)\n",
    "    # Remove numbers\n",
    "    texter = re.sub(r'\\d+', ' ', texter)\n",
    "    # Converting to Lowercase\n",
    "    texter = texter.lower()\n",
    "    # Remove punctuation\n",
    "    texter = re.sub(r'[^\\w\\s]', ' ', texter)\n",
    "    # Remove parentheses\n",
    "    texter = re.sub(r'\\([^)]*\\)', ' ', texter)\n",
    "    # Remove single quotes\n",
    "    texter = re.sub(r'\\'', ' ', texter)\n",
    "    # Substituting multiple spaces with single space\n",
    "    texter = re.sub(r'\\s+', ' ', texter, flags=re.I)\n",
    "\n",
    "    clean = re.compile('<.*?>')\n",
    "    texter = texter.encode('ascii', 'ignore').decode('ascii')\n",
    "    texter = re.sub(clean, '', texter)\n",
    "    if texter == \"\":\n",
    "        texter = \"\"\n",
    "    return texter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(clean)\n",
    "test['text'] = test['text'].apply(clean)\n",
    "val['text'] = val['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrBHC3acoAdY"
   },
   "source": [
    "# Logistic Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# vectorize\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # tfidf\n",
    "x_train = tfidf_vectorizer.fit_transform(train['text'])  # fit/transform on training data\n",
    "x_test = tfidf_vectorizer.transform(test['text'])  # only transform test data\n",
    "\n",
    "# train logreg\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "y_train = train['label']\n",
    "log_reg.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_test = test['label']\n",
    "y_pred = log_reg.predict(x_test)\n",
    "\n",
    "# eval\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpRIwyxLUbFP"
   },
   "source": [
    "# Embedding / Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "# load distilbert\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text_list, tokenizer, max_length=128):\n",
    "    \"\"\"Tokenizes and pads text data.\"\"\"\n",
    "    return tokenizer(\n",
    "        text_list,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize all datasets\n",
    "train_tokenized = tokenize_text(train['text'].tolist(), tokenizer)\n",
    "val_tokenized = tokenize_text(val['text'].tolist(), tokenizer)\n",
    "test_tokenized = tokenize_text(test['text'].tolist(), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfCQAEbuUbFR"
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class DistilBertForFakeNewsClassification(nn.Module):\n",
    "    def __init__(self, bert_model, num_labels):\n",
    "        super(DistilBertForFakeNewsClassification, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(768, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.fc(cls_embedding)\n",
    "        return logits\n",
    "\n",
    "# init model\n",
    "bert_classifier = DistilBertForFakeNewsClassification(distilbert_model, num_labels=1).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvU1Pj6cUbFS"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# optimzier/loss\n",
    "optimizer = AdamW(bert_classifier.parameters(), lr=2e-5)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess labels before  loop\n",
    "train_labels_tensor = torch.tensor(train['label'].values).unsqueeze(1).float().to(device)\n",
    "val_labels_tensor = torch.tensor(val['label'].values).unsqueeze(1).float().to(device)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    bert_classifier.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(0, len(train_tokenized['input_ids']), batch_size):\n",
    "        batch_input_ids = train_tokenized['input_ids'][i:i+batch_size].to(device)\n",
    "        batch_attention_mask = train_tokenized['attention_mask'][i:i+batch_size].to(device)\n",
    "        batch_labels = train_labels_tensor[i:i+batch_size]  # Use preprocessed labels\n",
    "\n",
    "        logits = bert_classifier(batch_input_ids, batch_attention_mask)\n",
    "        loss = loss_fn(logits, batch_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / (len(train_tokenized['input_ids']) / batch_size)\n",
    "\n",
    "    # validation\n",
    "    bert_classifier.eval()\n",
    "    val_logits_list = []\n",
    "    val_labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(val_tokenized['input_ids']), batch_size):\n",
    "            val_input_ids = val_tokenized['input_ids'][i:i+batch_size].to(device)\n",
    "            val_attention_mask = val_tokenized['attention_mask'][i:i+batch_size].to(device)\n",
    "            val_labels = val_labels_tensor[i:i+batch_size]  #  preprocessed labels\n",
    "\n",
    "            logits = bert_classifier(val_input_ids, val_attention_mask)\n",
    "            val_logits_list.extend(logits.cpu().numpy())\n",
    "            val_labels_list.extend(val_labels.cpu().numpy())  # to numpy for evaluation\n",
    "\n",
    "    val_predictions = (torch.tensor(val_logits_list).squeeze() > 0).float().numpy()\n",
    "    val_accuracy = accuracy_score(val_labels_list, val_predictions)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Training Loss: {avg_train_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "bert_classifier.eval()\n",
    "test_logits_list = []\n",
    "test_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_tokenized['input_ids']), batch_size):\n",
    "        test_input_ids = test_tokenized['input_ids'][i:i+batch_size].to(device)\n",
    "        test_attention_mask = test_tokenized['attention_mask'][i:i+batch_size].to(device)\n",
    "        test_labels = test['label'][i:i+batch_size].values\n",
    "\n",
    "        logits = bert_classifier(test_input_ids, test_attention_mask)\n",
    "        test_logits_list.extend(logits.cpu().numpy())\n",
    "        test_labels_list.extend(test_labels)\n",
    "\n",
    "# logits to predictions\n",
    "test_predictions = (torch.tensor(test_logits_list).squeeze() > 0).float().numpy()\n",
    "\n",
    "# test accuracy\n",
    "test_accuracy = accuracy_score(test_labels_list, test_predictions)\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels_list, test_predictions)\n",
    "\n",
    "# classification report\n",
    "clf_report = classification_report(test_labels_list, test_predictions, target_names=['Fake News', 'Real News'])\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(clf_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIrK9doUAr_1"
   },
   "source": [
    "# Combine Train + Val and Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine train and val set for final model\n",
    "final_train = pd.concat([train, val], ignore_index=True)\n",
    "\n",
    "final_tokenized = tokenize_text(final_train['text'].tolist(), tokenizer)\n",
    "final_labels_tensor = torch.tensor(final_train['label'].values).unsqueeze(1).float().to(device)\n",
    "\n",
    "# re init model\n",
    "bert_classifier = DistilBertForFakeNewsClassification(distilbert_model, num_labels=1).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labels_tensor = torch.tensor(final_train['label'].values).unsqueeze(1).float().to(device)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    bert_classifier.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(0, len(final_tokenized['input_ids']), batch_size):\n",
    "        batch_input_ids = final_tokenized['input_ids'][i:i+batch_size].to(device)\n",
    "        batch_attention_mask = final_tokenized['attention_mask'][i:i+batch_size].to(device)\n",
    "        batch_labels = final_labels_tensor[i:i+batch_size]  #  combined labels\n",
    "\n",
    "        logits = bert_classifier(batch_input_ids, batch_attention_mask)\n",
    "        loss = loss_fn(logits, batch_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / (len(final_tokenized['input_ids']) / batch_size)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Training Loss: {avg_train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O28CLg0yUbFT"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# eval on test set\n",
    "bert_classifier.eval()\n",
    "test_logits_list = []\n",
    "test_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_tokenized['input_ids']), batch_size):\n",
    "        test_input_ids = test_tokenized['input_ids'][i:i+batch_size].to(device)\n",
    "        test_attention_mask = test_tokenized['attention_mask'][i:i+batch_size].to(device)\n",
    "        test_labels = test['label'][i:i+batch_size].values\n",
    "\n",
    "        logits = bert_classifier(test_input_ids, test_attention_mask)\n",
    "        test_logits_list.extend(logits.cpu().numpy())\n",
    "        test_labels_list.extend(test_labels)\n",
    "\n",
    "# logits to predictions\n",
    "test_predictions = (torch.tensor(test_logits_list).squeeze() > 0).float().numpy()\n",
    "\n",
    "# test accuracy\n",
    "test_accuracy = accuracy_score(test_labels_list, test_predictions)\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels_list, test_predictions)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# classification report\n",
    "clf_report = classification_report(test_labels_list, test_predictions, target_names=['Fake News', 'Real News'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(clf_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_classifier.state_dict(), \"distilbert_fakenews_model.pth\")\n",
    "\n",
    "tokenizer.save_pretrained(\"distilbert_fakenews_tokenizer\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
